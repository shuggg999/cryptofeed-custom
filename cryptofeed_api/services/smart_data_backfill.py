"""
æ™ºèƒ½å†å²æ•°æ®è¡¥å……æœåŠ¡ - å‡çº§ç‰ˆ
åŸºäºç°æœ‰data_backfill.pyï¼Œæ·»åŠ ç²¾ç¡®ç¼ºå£æ£€æµ‹å’Œä¼˜å…ˆçº§åˆ†çº§åŠŸèƒ½
æ ¸å¿ƒåŸåˆ™ï¼šç¼ºä»€ä¹ˆè¡¥ä»€ä¹ˆï¼Œåˆ†çº§åˆ«å¤„ç†ï¼Œç¡®ä¿é‡åŒ–ç³»ç»Ÿæ•°æ®å®Œæ•´æ€§
"""

import asyncio
import logging
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

import clickhouse_connect
import yaml

from cryptofeed_api.monitor.config import config
from cryptofeed_api.services.data_normalizer import normalize_data

logger = logging.getLogger(__name__)


class GapType(Enum):
    """ç¼ºå£ç±»å‹æšä¸¾"""
    URGENT = "urgent"       # ç´§æ€¥ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰
    RECENT = "recent"       # è¿‘æœŸï¼ˆ1-24å°æ—¶ï¼‰
    HISTORICAL = "historical"  # å†å²ï¼ˆ24å°æ—¶ä»¥ä¸Šï¼‰
    NONE = "none"           # æ— ç¼ºå£


@dataclass
class PreciseGap:
    """ç²¾ç¡®ç¼ºå£ä¿¡æ¯"""
    symbol: str
    interval: str
    start_time: datetime
    end_time: datetime
    gap_type: GapType
    priority: int
    expected_records: int
    gap_duration_hours: float


@dataclass
class SmartBackfillTask:
    """æ™ºèƒ½å›å¡«ä»»åŠ¡ï¼ˆæ‰©å±•åŸæœ‰BackfillTaskï¼‰"""
    gap_log_id: int
    symbol: str
    data_type: str
    interval: Optional[str]
    start_time: datetime
    end_time: datetime
    gap_type: GapType
    priority: int
    expected_records: int
    status: str = 'pending'
    error_message: Optional[str] = None
    records_filled: int = 0


class SmartDataBackfillService:
    """æ™ºèƒ½æ•°æ®å›å¡«æœåŠ¡ - åœ¨ç°æœ‰é€»è¾‘åŸºç¡€ä¸Šæ·»åŠ æ™ºèƒ½åŠŸèƒ½"""

    def __init__(self, config_path: str = '/app/config/main.yaml'):
        """åˆå§‹åŒ–æ™ºèƒ½å›å¡«æœåŠ¡"""
        self.config = self._load_config(config_path)
        self.clickhouse_cfg = self.config['clickhouse']

        # ä¼˜å…ˆçº§æƒé‡é…ç½®
        self.priority_weights = {
            'time': {
                GapType.URGENT: 10,      # æœ€è¿‘1å°æ—¶
                GapType.RECENT: 7,       # 1-24å°æ—¶
                GapType.HISTORICAL: 3,   # 24å°æ—¶ä»¥ä¸Š
                GapType.NONE: 0
            },
            'interval': {
                '1m': 10, '5m': 8, '15m': 6, '30m': 5,
                '1h': 4, '4h': 3, '1d': 1
            }
        }

        # æ—¶é—´é˜ˆå€¼é…ç½®
        self.time_thresholds = {
            'urgent_hours': 1,    # 1å°æ—¶å†…ä¸ºç´§æ€¥
            'recent_hours': 24,   # 24å°æ—¶å†…ä¸ºè¿‘æœŸ
        }

        # ç¡®ä¿çŠ¶æ€è¡¨å­˜åœ¨
        self._ensure_status_tables()

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _ensure_status_tables(self):
        """ç¡®ä¿çŠ¶æ€ç®¡ç†è¡¨å­˜åœ¨"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            check_sql = "SELECT name FROM system.tables WHERE name = 'backfill_status'"
            result = client.query(check_sql)

            if not result.result_rows:
                logger.info("åˆ›å»ºæ™ºèƒ½å›å¡«çŠ¶æ€è¡¨...")
                with open('/Volumes/ç£ç›˜/Projects/cryptofeed/scripts/create_smart_backfill_tables.sql', 'r') as f:
                    sql_commands = f.read().split(';')
                    for sql in sql_commands:
                        if sql.strip():
                            try:
                                client.command(sql.strip())
                            except Exception as e:
                                logger.debug(f"SQLæ‰§è¡Œæç¤º: {e}")
        finally:
            client.close()

    async def smart_detect_and_fill(self, scenario: str = 'normal') -> Dict[str, any]:
        """
        æ™ºèƒ½æ£€æµ‹å’Œå›å¡«ä¸»å‡½æ•°
        scenario: 'normal', 'startup', 'network_recovery', 'manual_check'
        è¿”å›å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"ğŸ§  å¯åŠ¨æ™ºèƒ½æ•°æ®æ£€æµ‹å’Œå›å¡« (åœºæ™¯: {scenario})...")
        start_time = time.time()

        # 0. åœºæ™¯ç‰¹å®šçš„é¢„å¤„ç†
        scenario_info = await self._handle_scenario_preprocessing(scenario)

        # 1. å¿«é€ŸçŠ¶æ€æ£€æŸ¥
        status_summary = await self._quick_status_check()

        # 2. ç²¾ç¡®ç¼ºå£æ£€æµ‹ï¼ˆå¢å¼ºåœºæ™¯æ„ŸçŸ¥ï¼‰
        precise_gaps = await self._detect_precise_gaps(scenario)

        # 3. åœºæ™¯ç‰¹å®šçš„ç¼ºå£å¢å¼ºæ£€æµ‹
        additional_gaps = await self._detect_scenario_specific_gaps(scenario, scenario_info)
        precise_gaps.extend(additional_gaps)

        # 4. ä¼˜å…ˆçº§åˆ†ç±»
        classified_gaps = self._classify_gaps_by_priority(precise_gaps)

        # 5. æ›´æ–°çŠ¶æ€è¡¨
        await self._update_backfill_status(classified_gaps)

        # 6. æ‰§è¡Œåˆ†çº§å›å¡«
        results = await self._execute_prioritized_backfill(classified_gaps)

        duration = time.time() - start_time

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        return {
            'duration_seconds': duration,
            'scenario': scenario,
            'scenario_info': scenario_info,
            'status_summary': status_summary,
            'gaps_detected': len(precise_gaps),
            'gaps_by_priority': {
                'urgent': len([g for g in precise_gaps if g.gap_type == GapType.URGENT]),
                'recent': len([g for g in precise_gaps if g.gap_type == GapType.RECENT]),
                'historical': len([g for g in precise_gaps if g.gap_type == GapType.HISTORICAL])
            },
            'backfill_results': results
        }

    async def _quick_status_check(self) -> Dict[str, int]:
        """å¿«é€ŸçŠ¶æ€æ£€æŸ¥ - åªæŸ¥æ—¶é—´æˆ³ï¼Œä¸ä¸‹è½½æ•°æ®"""
        logger.info("âš¡ æ‰§è¡Œå¿«é€ŸçŠ¶æ€æ£€æŸ¥...")

        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            symbols = self.config['symbols']['custom_list']
            intervals = list(self.config['data_retention']['candles'].keys())

            total_sources = len(symbols) * len(intervals)
            complete_sources = 0
            partial_sources = 0
            missing_sources = 0

            for symbol in symbols:
                for interval in intervals:
                    status = await self._quick_check_single_source(
                        client, symbol, interval
                    )

                    if status == 'complete':
                        complete_sources += 1
                    elif status == 'partial':
                        partial_sources += 1
                    else:
                        missing_sources += 1

            logger.info(
                f"ğŸ“Š å¿«é€Ÿæ£€æŸ¥å®Œæˆ: "
                f"å®Œæ•´={complete_sources} | "
                f"éƒ¨åˆ†={partial_sources} | "
                f"ç¼ºå¤±={missing_sources}"
            )

            return {
                'total': total_sources,
                'complete': complete_sources,
                'partial': partial_sources,
                'missing': missing_sources
            }

        finally:
            client.close()

    async def _quick_check_single_source(
        self, client, symbol: str, interval: str
    ) -> str:
        """å¿«é€Ÿæ£€æŸ¥å•ä¸ªæ•°æ®æº"""
        # è·å–ä¿ç•™å¤©æ•°
        retention_days = self.config['data_retention']['candles'].get(interval, 30)
        now = datetime.now()
        expected_start = now - timedelta(days=retention_days)

        # æŸ¥è¯¢æœ€æ–°å’Œæœ€è€æ•°æ®
        sql = """
            SELECT
                MAX(timestamp) as latest,
                MIN(timestamp) as oldest,
                COUNT(*) as count
            FROM candles
            WHERE symbol = {symbol:String}
              AND interval = {interval:String}
        """

        result = client.query(sql, {'symbol': symbol, 'interval': interval})

        if not result.result_rows or result.result_rows[0][2] == 0:
            return 'missing'

        latest, oldest, count = result.result_rows[0]

        # ç®€å•åˆ¤æ–­æ•°æ®å®Œæ•´æ€§
        if latest and oldest:
            # æ£€æŸ¥æœ€æ–°æ•°æ®æ˜¯å¦åŠæ—¶
            time_since_latest = now - latest
            interval_minutes = self._get_interval_minutes(interval)
            expected_delay = timedelta(minutes=interval_minutes * 3)

            if time_since_latest > expected_delay:
                return 'partial'

            # æ£€æŸ¥å†å²æ•°æ®æ˜¯å¦å®Œæ•´
            if oldest > expected_start:
                return 'partial'

            return 'complete'

        return 'missing'

    async def _detect_precise_gaps(self, scenario: str = 'normal') -> List[PreciseGap]:
        """ç²¾ç¡®ç¼ºå£æ£€æµ‹ - é€æ—¶é—´æ®µæ£€æŸ¥ï¼Œæ‰¾å‡ºç¡®åˆ‡çš„ç¼ºå¤±éƒ¨åˆ†"""
        logger.info(f"ğŸ” æ‰§è¡Œç²¾ç¡®ç¼ºå£æ£€æµ‹ (åœºæ™¯: {scenario})...")

        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        all_gaps = []

        try:
            symbols = self.config['symbols']['custom_list']
            intervals = list(self.config['data_retention']['candles'].keys())

            for symbol in symbols:
                for interval in intervals:
                    gaps = await self._find_gaps_for_source(
                        client, symbol, interval, scenario
                    )
                    all_gaps.extend(gaps)

            logger.info(f"ğŸ¯ ç²¾ç¡®æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(all_gaps)} ä¸ªç¼ºå£")

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            all_gaps.sort(key=lambda g: g.priority, reverse=True)

            return all_gaps

        finally:
            client.close()

    async def _find_gaps_for_source(
        self, client, symbol: str, interval: str, scenario: str = 'normal'
    ) -> List[PreciseGap]:
        """ä¸ºå•ä¸ªæ•°æ®æºæŸ¥æ‰¾ç²¾ç¡®ç¼ºå£"""
        gaps = []
        now = datetime.now()

        # è·å–ä¿ç•™ç­–ç•¥
        retention_days = self.config['data_retention']['candles'].get(interval, 30)
        expected_start = now - timedelta(days=retention_days)
        interval_minutes = self._get_interval_minutes(interval)

        # æŸ¥è¯¢ç°æœ‰æ•°æ®çš„æ—¶é—´ç‚¹
        sql = """
            SELECT timestamp
            FROM candles
            WHERE symbol = {symbol:String}
              AND interval = {interval:String}
              AND timestamp >= {start:DateTime}
              AND timestamp <= {end:DateTime}
            ORDER BY timestamp
        """

        result = client.query(sql, {
            'symbol': symbol,
            'interval': interval,
            'start': expected_start,
            'end': now
        })

        if not result.result_rows:
            # å®Œå…¨æ²¡æœ‰æ•°æ®
            gap = self._create_gap(
                symbol, interval, expected_start, now, interval_minutes
            )
            gaps.append(gap)
            return gaps

        existing_times = [row[0] for row in result.result_rows]

        # æ£€æŸ¥å¼€å¤´ç¼ºå£
        first_time = existing_times[0]
        if first_time > expected_start:
            gap = self._create_gap(
                symbol, interval, expected_start, first_time, interval_minutes
            )
            gaps.append(gap)

        # æ£€æŸ¥ä¸­é—´ç¼ºå£
        for i in range(len(existing_times) - 1):
            current_time = existing_times[i]
            next_time = existing_times[i + 1]

            expected_next = current_time + timedelta(minutes=interval_minutes)

            # å¦‚æœä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹è·ç¦»å½“å‰æ—¶é—´è¶…è¿‡2ä¸ªé—´éš”ï¼Œè®¤ä¸ºæœ‰ç¼ºå£
            if next_time > expected_next + timedelta(minutes=interval_minutes):
                gap = self._create_gap(
                    symbol, interval, expected_next, next_time, interval_minutes
                )
                gaps.append(gap)

        # æ£€æŸ¥ç»“å°¾ç¼ºå£
        last_time = existing_times[-1]
        expected_latest = now - timedelta(minutes=interval_minutes * 2)

        if last_time < expected_latest:
            gap = self._create_gap(
                symbol, interval, last_time + timedelta(minutes=interval_minutes),
                now, interval_minutes
            )
            gaps.append(gap)

        return gaps

    def _create_gap(
        self, symbol: str, interval: str, start: datetime,
        end: datetime, interval_minutes: int
    ) -> PreciseGap:
        """åˆ›å»ºç¼ºå£å¯¹è±¡"""
        duration_hours = (end - start).total_seconds() / 3600

        # åˆ¤æ–­ç¼ºå£ç±»å‹
        if duration_hours <= self.time_thresholds['urgent_hours']:
            gap_type = GapType.URGENT
        elif duration_hours <= self.time_thresholds['recent_hours']:
            gap_type = GapType.RECENT
        else:
            gap_type = GapType.HISTORICAL

        # è®¡ç®—ä¼˜å…ˆçº§
        priority = self._calculate_priority(gap_type, interval, duration_hours)

        # ä¼°ç®—æœŸæœ›è®°å½•æ•°
        expected_records = int(duration_hours * 60 / interval_minutes)

        return PreciseGap(
            symbol=symbol,
            interval=interval,
            start_time=start,
            end_time=end,
            gap_type=gap_type,
            priority=priority,
            expected_records=expected_records,
            gap_duration_hours=duration_hours
        )

    def _calculate_priority(
        self, gap_type: GapType, interval: str, duration_hours: float
    ) -> int:
        """è®¡ç®—å›å¡«ä¼˜å…ˆçº§"""
        # æ—¶é—´æƒé‡
        time_weight = self.priority_weights['time'].get(gap_type, 1)

        # é—´éš”æƒé‡
        interval_weight = self.priority_weights['interval'].get(interval, 1)

        # æŒç»­æ—¶é—´æƒé‡ï¼ˆè¶Šé•¿ä¼˜å…ˆçº§è¶Šé«˜ï¼Œä½†æœ‰ä¸Šé™ï¼‰
        duration_weight = min(5, max(1, duration_hours / 24))

        # ç»¼åˆä¼˜å…ˆçº§ï¼ˆ1-10åˆ†ï¼‰
        priority = min(10, max(1,
            time_weight * 0.6 + interval_weight * 0.3 + duration_weight * 0.1
        ))

        return int(priority)

    def _classify_gaps_by_priority(self, gaps: List[PreciseGap]) -> Dict[str, List[PreciseGap]]:
        """æŒ‰ä¼˜å…ˆçº§åˆ†ç±»ç¼ºå£"""
        classified = {
            'urgent': [],
            'recent': [],
            'historical': []
        }

        for gap in gaps:
            if gap.gap_type == GapType.URGENT:
                classified['urgent'].append(gap)
            elif gap.gap_type == GapType.RECENT:
                classified['recent'].append(gap)
            else:
                classified['historical'].append(gap)

        return classified

    async def _update_backfill_status(self, classified_gaps: Dict[str, List[PreciseGap]]):
        """æ›´æ–°å›å¡«çŠ¶æ€è¡¨"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            # è®°å½•æ£€æµ‹åˆ°çš„ç¼ºå£
            for gap_type, gaps in classified_gaps.items():
                for gap in gaps:
                    # æ’å…¥ç¼ºå£æ£€æµ‹æ—¥å¿—
                    sql = """
                        INSERT INTO gap_detection_log (
                            log_id, symbol, interval, gap_start, gap_end,
                            gap_type, priority, records_expected, detection_time
                        ) VALUES (
                            {log_id:UInt64}, {symbol:String}, {interval:String},
                            {start:DateTime}, {end:DateTime}, {gap_type:String},
                            {priority:Int8}, {expected:UInt32}, {detection:DateTime}
                        )
                    """

                    client.command(sql, {
                        'log_id': int(time.time() * 1000000),  # å¾®ç§’æ—¶é—´æˆ³ä½œä¸ºID
                        'symbol': gap.symbol,
                        'interval': gap.interval,
                        'start': gap.start_time,
                        'end': gap.end_time,
                        'gap_type': gap.gap_type.value,
                        'priority': gap.priority,
                        'expected': gap.expected_records,
                        'detection': datetime.now()
                    })

        finally:
            client.close()

    async def _execute_prioritized_backfill(
        self, classified_gaps: Dict[str, List[PreciseGap]]
    ) -> Dict[str, any]:
        """æ‰§è¡Œåˆ†çº§å›å¡« - ç´§æ€¥ä¼˜å…ˆï¼Œå†å²å»¶å"""
        results = {
            'urgent_completed': 0,
            'recent_completed': 0,
            'historical_completed': 0,
            'total_records_filled': 0,
            'errors': []
        }

        # 1. ç«‹å³å¤„ç†ç´§æ€¥ç¼ºå£
        if classified_gaps['urgent']:
            logger.info(f"ğŸš¨ ç«‹å³å¤„ç† {len(classified_gaps['urgent'])} ä¸ªç´§æ€¥ç¼ºå£")
            urgent_results = await self._fill_gaps_batch(classified_gaps['urgent'])
            results['urgent_completed'] = urgent_results['completed']
            results['total_records_filled'] += urgent_results['records_filled']
            results['errors'].extend(urgent_results['errors'])

        # 2. å¤„ç†è¿‘æœŸç¼ºå£
        if classified_gaps['recent']:
            logger.info(f"â° å¤„ç† {len(classified_gaps['recent'])} ä¸ªè¿‘æœŸç¼ºå£")
            recent_results = await self._fill_gaps_batch(classified_gaps['recent'])
            results['recent_completed'] = recent_results['completed']
            results['total_records_filled'] += recent_results['records_filled']
            results['errors'].extend(recent_results['errors'])

        # 3. å»¶åå¤„ç†å†å²ç¼ºå£ï¼ˆå¯é€‰ï¼Œæ ¹æ®ç³»ç»Ÿè´Ÿè½½ï¼‰
        if classified_gaps['historical']:
            logger.info(f"ğŸ“š å»¶åå¤„ç† {len(classified_gaps['historical'])} ä¸ªå†å²ç¼ºå£")
            # å¯ä»¥é€‰æ‹©ç«‹å³å¤„ç†æˆ–æ”¾å…¥é˜Ÿåˆ—ç¨åå¤„ç†
            historical_results = await self._fill_gaps_batch(classified_gaps['historical'][:5])  # é™åˆ¶æ•°é‡
            results['historical_completed'] = historical_results['completed']
            results['total_records_filled'] += historical_results['records_filled']
            results['errors'].extend(historical_results['errors'])

        return results

    async def _fill_gaps_batch(self, gaps: List[PreciseGap]) -> Dict[str, any]:
        """æ‰¹é‡å¡«å……ç¼ºå£ - å¤ç”¨ç°æœ‰çš„APIè°ƒç”¨é€»è¾‘"""
        results = {
            'completed': 0,
            'records_filled': 0,
            'errors': []
        }

        for gap in gaps:
            try:
                # è°ƒç”¨ç°æœ‰çš„å›å¡«é€»è¾‘ï¼ˆå¤ç”¨åŸæœ‰çš„APIè°ƒç”¨ä»£ç ï¼‰
                records_filled = await self._fill_single_gap(gap)

                results['completed'] += 1
                results['records_filled'] += records_filled

                logger.info(
                    f"âœ… å¡«å……å®Œæˆ: {gap.symbol} {gap.interval} "
                    f"[{gap.start_time.strftime('%m-%d %H:%M')} - "
                    f"{gap.end_time.strftime('%m-%d %H:%M')}] "
                    f"å¡«å…… {records_filled} æ¡è®°å½•"
                )

            except Exception as e:
                error_msg = f"å¡«å……å¤±è´¥: {gap.symbol} {gap.interval} - {str(e)}"
                logger.error(error_msg)
                results['errors'].append(error_msg)

        return results

    async def _fill_single_gap(self, gap: PreciseGap) -> int:
        """å¡«å……å•ä¸ªç¼ºå£ - å¤ç”¨ç°æœ‰çš„Binance APIé€»è¾‘"""
        # è¿™é‡Œå¤ç”¨åŸæœ‰data_backfill.pyä¸­çš„APIè°ƒç”¨ä»£ç 
        # ä¿æŒæ‰€æœ‰ç°æœ‰çš„è½¬æ¢ã€é”™è¯¯å¤„ç†ã€æ•°æ®æ’å…¥é€»è¾‘ä¸å˜

        # è½¬æ¢ä¸ºBinanceæ ¼å¼ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
        binance_symbol = self._convert_symbol_for_binance(gap.symbol)
        binance_interval = self._convert_interval_for_binance(gap.interval)

        start_ms = int(gap.start_time.timestamp() * 1000)
        end_ms = int(gap.end_time.timestamp() * 1000)

        total_inserted = 0
        current_start = start_ms

        while current_start < end_ms:
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸæ—¶é—´
            current_end = min(current_start + (7 * 24 * 60 * 60 * 1000), end_ms)  # 7å¤©ä¸€æ‰¹

            try:
                # è°ƒç”¨Binance APIï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
                url = "https://fapi.binance.com/fapi/v1/klines"
                params = {
                    'symbol': binance_symbol,
                    'interval': binance_interval,
                    'startTime': current_start,
                    'endTime': current_end,
                    'limit': 1500
                }

                response = requests.get(url, params=params, timeout=10)
                response.raise_for_status()

                klines_data = response.json()

                if klines_data:
                    # æ’å…¥æ•°æ®ï¼ˆå¤ç”¨åŸæœ‰çš„æ•°æ®å¤„ç†é€»è¾‘ï¼‰
                    inserted = await self._insert_klines_data(
                        klines_data, gap.symbol, gap.interval
                    )
                    total_inserted += inserted

                # APIé™åˆ¶å»¶è¿Ÿ
                await asyncio.sleep(0.05)

                current_start = current_end + 1

            except Exception as e:
                logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                raise

        return total_inserted

    async def _insert_klines_data(
        self, klines_data: List, symbol: str, interval: str
    ) -> int:
        """æ’å…¥Kçº¿æ•°æ® - å¤ç”¨åŸæœ‰çš„æ•°æ®æ’å…¥é€»è¾‘"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            insert_data = []
            for kline in klines_data:
                open_time = datetime.fromtimestamp(kline[0] / 1000)

                # å¤ç”¨åŸæœ‰çš„æ•°æ®ç»“æ„
                data_dict = {
                    'timestamp': open_time,
                    'exchange': 'binance',
                    'symbol': symbol,
                    'interval': interval,
                    'open': float(kline[1]),
                    'high': float(kline[2]),
                    'low': float(kline[3]),
                    'close': float(kline[4]),
                    'volume': float(kline[5])
                }

                # æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
                normalized_data = normalize_data(data_dict, 'candles')
                insert_data.append(list(normalized_data.values()))

            if insert_data:
                client.insert(
                    'candles',
                    insert_data,
                    column_names=list(normalized_data.keys()) if normalized_data else []
                )

            return len(insert_data)

        finally:
            client.close()

    def _convert_symbol_for_binance(self, symbol: str) -> str:
        """è½¬æ¢ç¬¦å·æ ¼å¼ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
        if symbol.endswith('-PERP'):
            return symbol.replace('-USDT-PERP', 'USDT').replace('-', '')
        return symbol.replace('-', '')

    def _convert_interval_for_binance(self, interval: str) -> str:
        """è½¬æ¢æ—¶é—´é—´éš”æ ¼å¼ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
        mapping = {
            '1m': '1m', '5m': '5m', '15m': '15m', '30m': '30m',
            '1h': '1h', '4h': '4h', '1d': '1d'
        }
        return mapping.get(interval, interval)

    def _get_interval_minutes(self, interval: str) -> int:
        """è·å–æ—¶é—´é—´éš”çš„åˆ†é’Ÿæ•°"""
        mapping = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30,
            '1h': 60, '4h': 240, '1d': 1440
        }
        return mapping.get(interval, 60)

    async def _handle_scenario_preprocessing(self, scenario: str) -> Dict[str, any]:
        """åœºæ™¯ç‰¹å®šçš„é¢„å¤„ç†"""
        scenario_info = {'scenario': scenario, 'timestamp': datetime.now()}

        if scenario == 'startup':
            # é‡å¯åœºæ™¯: è·å–æœ€åä¸€æ¬¡æ£€æŸ¥æ—¶é—´
            last_check = await self._get_last_check_time()
            scenario_info['last_check_time'] = last_check
            scenario_info['downtime_duration'] = (datetime.now() - last_check).total_seconds() / 3600 if last_check else None
            logger.info(f"ğŸ”„ é‡å¯åœºæ™¯æ£€æµ‹: ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´ {last_check}, åœæœºæ—¶é•¿ {scenario_info['downtime_duration']:.1f}å°æ—¶")

        elif scenario == 'network_recovery':
            # ç½‘ç»œæ¢å¤åœºæ™¯: æ£€æŸ¥WebSocketè¿æ¥çŠ¶æ€
            network_status = await self._check_network_disruption_status()
            scenario_info['network_status'] = network_status
            logger.info(f"ğŸŒ ç½‘ç»œæ¢å¤æ£€æµ‹: {network_status}")

        elif scenario == 'manual_check':
            # æ‰‹åŠ¨æ£€æŸ¥åœºæ™¯: æ£€æŸ¥äººä¸ºæ•°æ®å˜åŠ¨
            data_anomalies = await self._detect_data_anomalies()
            scenario_info['data_anomalies'] = data_anomalies
            logger.info(f"ğŸ•µï¸ æ‰‹åŠ¨æ£€æŸ¥åœºæ™¯: å‘ç° {len(data_anomalies)} ä¸ªæ•°æ®å¼‚å¸¸")

        return scenario_info

    async def _detect_scenario_specific_gaps(
        self, scenario: str, scenario_info: Dict[str, any]
    ) -> List[PreciseGap]:
        """åœºæ™¯ç‰¹å®šçš„ç¼ºå£æ£€æµ‹"""
        additional_gaps = []

        if scenario == 'startup':
            # é‡å¯åœºæ™¯: é‡ç‚¹æ£€æŸ¥åœæœºæœŸé—´çš„ç¼ºå£
            additional_gaps = await self._detect_startup_gaps(scenario_info)

        elif scenario == 'network_recovery':
            # ç½‘ç»œæ¢å¤åœºæ™¯: æ£€æŸ¥ç½‘ç»œä¸­æ–­æœŸé—´çš„ç¼ºå£
            additional_gaps = await self._detect_network_gaps(scenario_info)

        elif scenario == 'manual_check':
            # æ‰‹åŠ¨æ£€æŸ¥åœºæ™¯: æ£€æŸ¥äººä¸ºæ“ä½œå¯¼è‡´çš„ç¼ºå£
            additional_gaps = await self._detect_manual_deletion_gaps(scenario_info)

        logger.info(f"ğŸ¯ åœºæ™¯ç‰¹å®šæ£€æµ‹å®Œæˆ: {scenario} - å‘ç° {len(additional_gaps)} ä¸ªé¢å¤–ç¼ºå£")
        return additional_gaps

    async def _get_last_check_time(self) -> Optional[datetime]:
        """è·å–æœ€åä¸€æ¬¡æ£€æŸ¥æ—¶é—´"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            sql = "SELECT MAX(last_check_time) FROM backfill_status"
            result = client.query(sql)

            if result.result_rows and result.result_rows[0][0]:
                return result.result_rows[0][0]

            # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›24å°æ—¶å‰
            return datetime.now() - timedelta(hours=24)

        finally:
            client.close()

    async def _check_network_disruption_status(self) -> Dict[str, any]:
        """æ£€æŸ¥ç½‘ç»œä¸­æ–­çŠ¶æ€"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        try:
            # æ£€æŸ¥å®æ—¶ç›‘æ§è¡¨ä¸­çš„è¿ç»­ç¼ºå£æ•°
            sql = """
                SELECT symbol, interval, consecutive_gaps, last_websocket_time, status
                FROM real_time_monitor
                WHERE consecutive_gaps > 2 OR status != 'normal'
            """

            result = client.query(sql)

            disrupted_sources = []
            for row in result.result_rows:
                disrupted_sources.append({
                    'symbol': row[0],
                    'interval': row[1],
                    'consecutive_gaps': row[2],
                    'last_websocket_time': row[3],
                    'status': row[4]
                })

            return {
                'disrupted_count': len(disrupted_sources),
                'disrupted_sources': disrupted_sources
            }

        finally:
            client.close()

    async def _detect_data_anomalies(self) -> List[Dict[str, any]]:
        """æ£€æµ‹æ•°æ®å¼‚å¸¸ï¼ˆå¯èƒ½çš„æ‰‹åŠ¨åˆ é™¤ï¼‰"""
        client = clickhouse_connect.get_client(
            host=self.clickhouse_cfg['host'],
            port=self.clickhouse_cfg['port'],
            username=self.clickhouse_cfg['user'],
            password=self.clickhouse_cfg.get('password', ''),
            database=self.clickhouse_cfg['database']
        )

        anomalies = []

        try:
            symbols = self.config['symbols']['custom_list']
            intervals = list(self.config['data_retention']['candles'].keys())

            for symbol in symbols:
                for interval in intervals:
                    # æ£€æŸ¥æœ€è¿‘çš„æ•°æ®å¯†åº¦æ˜¯å¦å¼‚å¸¸
                    sql = """
                        SELECT
                            COUNT(*) as recent_count,
                            MIN(timestamp) as min_time,
                            MAX(timestamp) as max_time
                        FROM candles
                        WHERE symbol = {symbol:String}
                          AND interval = {interval:String}
                          AND timestamp >= {check_time:DateTime}
                    """

                    check_time = datetime.now() - timedelta(hours=6)
                    result = client.query(sql, {
                        'symbol': symbol,
                        'interval': interval,
                        'check_time': check_time
                    })

                    if result.result_rows:
                        count, min_time, max_time = result.result_rows[0]

                        if count > 0 and min_time and max_time:
                            # è®¡ç®—æœŸæœ›çš„æ•°æ®æ•°é‡
                            interval_minutes = self._get_interval_minutes(interval)
                            expected_count = 6 * 60 / interval_minutes  # 6å°æ—¶çš„æœŸæœ›æ•°é‡

                            # å¦‚æœå®é™…æ•°é‡æ˜æ˜¾å°‘äºæœŸæœ›ï¼Œå¯èƒ½æ˜¯æ‰‹åŠ¨åˆ é™¤
                            if count < expected_count * 0.7:  # ä½äº70%è®¤ä¸ºå¼‚å¸¸
                                anomalies.append({
                                    'symbol': symbol,
                                    'interval': interval,
                                    'actual_count': count,
                                    'expected_count': int(expected_count),
                                    'missing_ratio': 1 - (count / expected_count),
                                    'time_range': {'start': min_time, 'end': max_time}
                                })

            return anomalies

        finally:
            client.close()

    async def _detect_startup_gaps(self, scenario_info: Dict[str, any]) -> List[PreciseGap]:
        """æ£€æµ‹é‡å¯åœºæ™¯çš„ç¼ºå£"""
        gaps = []
        last_check = scenario_info.get('last_check_time')

        if not last_check:
            return gaps

        # é‡ç‚¹æ£€æŸ¥åœæœºæœŸé—´çš„ç¼ºå£
        symbols = self.config['symbols']['custom_list']
        intervals = list(self.config['data_retention']['candles'].keys())
        now = datetime.now()

        for symbol in symbols:
            for interval in intervals:
                # æ£€æŸ¥åœæœºæœŸé—´æ˜¯å¦æœ‰ç¼ºå£
                gap = self._create_gap(
                    symbol, interval, last_check, now,
                    self._get_interval_minutes(interval)
                )

                # é‡å¯ç¼ºå£ä¼˜å…ˆçº§æé«˜
                gap.priority = min(10, gap.priority + 2)
                gaps.append(gap)

        logger.info(f"ğŸ”„ é‡å¯ç¼ºå£æ£€æµ‹: åœæœºæœŸé—´ {last_check} - {now}, å‘ç° {len(gaps)} ä¸ªç¼ºå£")
        return gaps

    async def _detect_network_gaps(self, scenario_info: Dict[str, any]) -> List[PreciseGap]:
        """æ£€æµ‹ç½‘ç»œä¸­æ–­çš„ç¼ºå£"""
        gaps = []
        network_status = scenario_info.get('network_status', {})

        for source in network_status.get('disrupted_sources', []):
            symbol = source['symbol']
            interval = source['interval']
            last_ws_time = source.get('last_websocket_time')

            if last_ws_time:
                # ä»æœ€åWebSocketæ—¶é—´åˆ°ç°åœ¨çš„ç¼ºå£
                gap = self._create_gap(
                    symbol, interval, last_ws_time, datetime.now(),
                    self._get_interval_minutes(interval)
                )

                # ç½‘ç»œä¸­æ–­ç¼ºå£ä¼˜å…ˆçº§æé«˜
                gap.priority = min(10, gap.priority + 1)
                gaps.append(gap)

        logger.info(f"ğŸŒ ç½‘ç»œä¸­æ–­ç¼ºå£æ£€æµ‹: å‘ç° {len(gaps)} ä¸ªç¼ºå£")
        return gaps

    async def _detect_manual_deletion_gaps(self, scenario_info: Dict[str, any]) -> List[PreciseGap]:
        """æ£€æµ‹æ‰‹åŠ¨åˆ é™¤çš„ç¼ºå£"""
        gaps = []
        anomalies = scenario_info.get('data_anomalies', [])

        for anomaly in anomalies:
            symbol = anomaly['symbol']
            interval = anomaly['interval']
            time_range = anomaly['time_range']

            # ä¸ºæ•°æ®å¼‚å¸¸çš„æ—¶é—´èŒƒå›´åˆ›å»ºç¼ºå£
            gap = self._create_gap(
                symbol, interval,
                time_range['start'], time_range['end'],
                self._get_interval_minutes(interval)
            )

            # æ‰‹åŠ¨åˆ é™¤ç¼ºå£ä¼˜å…ˆçº§ä¸­ç­‰
            gap.priority = max(5, gap.priority)
            gaps.append(gap)

        logger.info(f"ğŸ•µï¸ æ‰‹åŠ¨åˆ é™¤ç¼ºå£æ£€æµ‹: å‘ç° {len(gaps)} ä¸ªç¼ºå£")
        return gaps


# ä¾›ä¸»ç¨‹åºè°ƒç”¨çš„æ¥å£å‡½æ•°
async def smart_data_integrity_check(scenario: str = 'normal'):
    """
    æ™ºèƒ½æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ï¼ˆä¾›ä¸»ç¨‹åºè°ƒç”¨ï¼‰
    scenario: 'normal', 'startup', 'network_recovery', 'manual_check'
    """
    service = SmartDataBackfillService()
    return await service.smart_detect_and_fill(scenario)

# åœºæ™¯ç‰¹å®šçš„ä¾¿æ·æ¥å£
async def startup_data_check():
    """é‡å¯æ—¶çš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥"""
    return await smart_data_integrity_check('startup')

async def network_recovery_check():
    """ç½‘ç»œæ¢å¤åçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥"""
    return await smart_data_integrity_check('network_recovery')

async def manual_data_audit():
    """æ‰‹åŠ¨æ•°æ®å®¡è®¡æ£€æŸ¥"""
    return await smart_data_integrity_check('manual_check')


if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½å›å¡«ç³»ç»Ÿ
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    async def test():
        result = await smart_data_integrity_check()
        print(f"æ™ºèƒ½å›å¡«å®Œæˆ: {result}")

    asyncio.run(test())