"""
å†å²æ•°æ®è¡¥å……æœåŠ¡ - ClickHouseç‰ˆæœ¬
è‡ªåŠ¨è¡¥å……ClickHouseæ•°æ®åº“ä¸­ç¼ºå¤±çš„å†å²æ•°æ®
"""

import asyncio
import logging
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

import clickhouse_connect
import requests

from cryptofeed_api.monitor.config import config
from cryptofeed_api.services.data_normalizer import normalize_data

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


@dataclass
class BackfillTask:
    """æ•°æ®è¡¥å……ä»»åŠ¡"""

    gap_log_id: int
    symbol: str
    data_type: str  # 'candles', 'trades', 'funding'
    interval: Optional[str]  # ä»…ç”¨äºcandles
    start_time: datetime
    end_time: datetime
    status: str = "pending"
    error_message: Optional[str] = None
    records_filled: int = 0


@dataclass
class BackfillResult:
    """æ•°æ®è¡¥å……ç»“æœ"""

    task: BackfillTask
    success: bool
    records_added: int
    error_message: Optional[str] = None
    duration_seconds: float = 0


class DataBackfillService:
    """ClickHouseæ•°æ®è¡¥å……æœåŠ¡"""

    def __init__(self, max_concurrent_tasks: int = 3):
        self.max_concurrent_tasks = max_concurrent_tasks
        self._active_tasks = 0

        # ClickHouseè¿æ¥é…ç½® - ä»ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶è¯»å–
        import os

        clickhouse_config = config.get("clickhouse", {})
        self.ch_config = {
            "host": os.getenv("CLICKHOUSE_HOST", clickhouse_config.get("host", "localhost")),
            "port": int(os.getenv("CLICKHOUSE_PORT", clickhouse_config.get("port", 8123))),
            "user": os.getenv("CLICKHOUSE_USER", clickhouse_config.get("user", "default")),
            "password": os.getenv("CLICKHOUSE_PASSWORD", clickhouse_config.get("password", "password123")),
            "database": os.getenv("CLICKHOUSE_DATABASE", clickhouse_config.get("database", "cryptofeed")),
        }

    def detect_data_gaps(self, symbols: List[str], lookback_days: int = None) -> List[BackfillTask]:
        """
        æ•°æ®ç¼ºå£æ£€æµ‹ - æŒ‰é…ç½®çš„å›å¡«ç­–ç•¥æ£€æµ‹

        Args:
            symbols: è¦æ£€æŸ¥çš„äº¤æ˜“å¯¹åˆ—è¡¨
            lookback_days: æ£€æŸ¥æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆå¿½ç•¥ï¼Œä½¿ç”¨é…ç½®ä¸­çš„æŒ‰æ—¶é—´é—´éš”è®¾ç½®ï¼‰

        Returns:
            æ£€æµ‹åˆ°çš„ç¼ºå£ä»»åŠ¡åˆ—è¡¨
        """
        ch_client = clickhouse_connect.get_client(**self.ch_config)
        tasks = []
        now = datetime.utcnow()

        intervals = ["1d", "4h", "30m", "5m", "1m"]

        # ä»ç»Ÿä¸€çš„æ•°æ®ä¿ç•™ç­–ç•¥é…ç½®ä¸­è·å–æ¯ä¸ªæ—¶é—´é—´éš”çš„å›å¡«å¤©æ•°
        retention_config = config.get("data_retention", {})
        candles_retention = retention_config.get("candles", {})

        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆä¸data_retentionä¿æŒä¸€è‡´ï¼‰
        lookback_by_interval = candles_retention or {
            "1d": 1095,  # 3å¹´
            "4h": 730,  # 2å¹´
            "30m": 365,  # 1å¹´
            "5m": 90,  # 90å¤©
            "1m": 30,  # 30å¤©
        }

        logger.info(f"ğŸ“‹ ä½¿ç”¨ç»Ÿä¸€æ•°æ®ä¿ç•™ç­–ç•¥è¿›è¡Œå›å¡«: {lookback_by_interval}")

        try:
            for symbol in symbols:
                for interval in intervals:
                    # è·å–è¯¥æ—¶é—´é—´éš”çš„å›å¡«å¤©æ•°
                    interval_lookback_days = lookback_by_interval.get(interval, 7)
                    logger.info(f"æ£€æŸ¥ {symbol} {interval}ï¼Œå›å¡«èŒƒå›´ï¼š{interval_lookback_days} å¤©")

                    # æŸ¥æ‰¾è¯¥äº¤æ˜“å¯¹å’Œæ—¶é—´é—´éš”çš„æœ€æ–°æ•°æ®æ—¶é—´
                    sql = """
                        SELECT MAX(timestamp) as latest_time
                        FROM candles
                        WHERE symbol = {symbol:String} AND interval = {interval:String}
                    """

                    result = ch_client.query(sql, {"symbol": symbol, "interval": interval})

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ä¸”æ•°æ®ä¸ä¸ºNULL
                    if result.result_rows and result.result_rows[0][0] is not None:
                        latest_time = result.result_rows[0][0]

                        # ç¡®ä¿latest_timeæ˜¯æœ‰æ•ˆçš„æ—¥æœŸï¼ˆä¸æ˜¯1970å¹´ï¼‰
                        if latest_time.year < 2000:
                            logger.warning(f"å‘ç°æ— æ•ˆçš„æ—¶é—´æˆ³: {latest_time}, å½“ä½œæ— æ•°æ®å¤„ç†")
                            latest_time = None

                    else:
                        latest_time = None

                    if latest_time is not None:
                        # æœ‰æ•°æ®çš„æƒ…å†µ
                        # è®¡ç®—åº”è¯¥å›å¡«åˆ°çš„å¼€å§‹æ—¶é—´
                        target_start_time = now - timedelta(days=interval_lookback_days)

                        # å¦‚æœæœ€æ–°æ•°æ®æ—¶é—´æ—©äºç›®æ ‡å¼€å§‹æ—¶é—´ï¼Œè¯´æ˜å†å²æ•°æ®ä¸å®Œæ•´
                        if latest_time < target_start_time:
                            logger.info(
                                f"æ•°æ®ä¸å®Œæ•´: {symbol} {interval} æœ€æ–°æ•°æ® {latest_time}ï¼Œåº”è¯¥ä» {target_start_time} å¼€å§‹"
                            )

                            task = BackfillTask(
                                gap_log_id=0,
                                symbol=symbol,
                                data_type="candles",
                                interval=interval,
                                start_time=target_start_time,
                                end_time=latest_time,  # è¡¥å……åˆ°å·²æœ‰æ•°æ®å¼€å§‹çš„ä½ç½®
                            )
                            tasks.append(task)
                            logger.info(f"éœ€è¦å›å¡«: {symbol} {interval} ä» {target_start_time} åˆ° {latest_time}")

                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ€æ–°çš„ç¼ºå£ï¼ˆä»æœ€æ–°æ•°æ®åˆ°ç°åœ¨ï¼‰
                        time_since_latest = now - latest_time

                        # æ ¹æ®æ—¶é—´é—´éš”è°ƒæ•´æœ€æ–°ç¼ºå£æ£€æŸ¥é˜ˆå€¼
                        interval_minutes = self._get_interval_minutes(interval)
                        threshold_seconds = max(interval_minutes * 60 * 3, 3600)  # è‡³å°‘3ä¸ªé—´éš”æˆ–1å°æ—¶

                        if time_since_latest.total_seconds() > threshold_seconds:
                            # åªå›å¡«ä»æœ€æ–°æ•°æ®åˆ°ç°åœ¨çš„ç¼ºå£ï¼Œä¸å›å¡«å†å²
                            task = BackfillTask(
                                gap_log_id=0,
                                symbol=symbol,
                                data_type="candles",
                                interval=interval,
                                start_time=latest_time,
                                end_time=now,
                            )
                            tasks.append(task)
                            logger.info(
                                f"æœ€æ–°ç¼ºå£: {symbol} {interval} ä» {latest_time} åˆ° {now} (å»¶è¿Ÿ{time_since_latest.total_seconds()/3600:.1f}å°æ—¶)"
                            )
                        else:
                            logger.info(f"æ•°æ®å®Œæ•´: {symbol} {interval} æœ€æ–°æ•°æ® {latest_time}ï¼Œæ— éœ€å›å¡«")

                    else:
                        # å®Œå…¨æ²¡æœ‰æ•°æ®ï¼Œä»æŒ‡å®šå¤©æ•°å‰å¼€å§‹å›å¡«
                        start_time = now - timedelta(days=interval_lookback_days)
                        task = BackfillTask(
                            gap_log_id=0,
                            symbol=symbol,
                            data_type="candles",
                            interval=interval,
                            start_time=start_time,
                            end_time=now,
                        )
                        tasks.append(task)
                        logger.info(
                            f"å®Œå…¨æ— æ•°æ®: {symbol} {interval} ä» {start_time} åˆ° {now} ({interval_lookback_days}å¤©)"
                        )

            logger.info(f"æ£€æµ‹åˆ° {len(tasks)} ä¸ªæ•°æ®ç¼ºå£")
            return tasks

        finally:
            ch_client.close()

    def _find_precise_gaps(
        self, client, symbol: str, interval: str, start_time: datetime, end_time: datetime, interval_min: int
    ) -> List[Tuple[datetime, datetime]]:
        """
        ç²¾ç¡®æŸ¥æ‰¾æ•°æ®ç¼ºå£
        """
        # è·å–ç°æœ‰æ•°æ®çš„æ—¶é—´ç‚¹
        sql = """
            SELECT timestamp
            FROM candles
            WHERE symbol = {symbol:String}
              AND interval = {interval:String}
              AND timestamp >= {start_time:DateTime}
              AND timestamp <= {end_time:DateTime}
            ORDER BY timestamp
        """

        result = client.query(
            sql, {"symbol": symbol, "interval": interval, "start_time": start_time, "end_time": end_time}
        )

        if not result.result_rows:
            # å®Œå…¨æ²¡æœ‰æ•°æ®
            return [(start_time, end_time)]

        existing_timestamps = [row[0] for row in result.result_rows]
        gaps = []

        # ç”ŸæˆæœŸæœ›çš„æ—¶é—´åºåˆ—
        expected_timestamps = []
        current = start_time
        while current <= end_time:
            expected_timestamps.append(current)
            current += timedelta(minutes=interval_min)

        # æ‰¾å‡ºç¼ºå¤±çš„æ—¶é—´æ®µ
        existing_set = set(existing_timestamps)
        missing_timestamps = [ts for ts in expected_timestamps if ts not in existing_set]

        if not missing_timestamps:
            return []  # æ²¡æœ‰ç¼ºå£

        # å°†è¿ç»­çš„ç¼ºå¤±æ—¶é—´åˆå¹¶ä¸ºç¼ºå£èŒƒå›´
        gaps = []
        gap_start = missing_timestamps[0]
        gap_end = missing_timestamps[0]

        for i in range(1, len(missing_timestamps)):
            current_ts = missing_timestamps[i]
            expected_next = gap_end + timedelta(minutes=interval_min)

            if current_ts == expected_next:
                # è¿ç»­ç¼ºå¤±ï¼Œæ‰©å±•å½“å‰ç¼ºå£
                gap_end = current_ts
            else:
                # æ–°çš„ç¼ºå£å¼€å§‹
                gaps.append((gap_start, gap_end + timedelta(minutes=interval_min)))
                gap_start = current_ts
                gap_end = current_ts

        # æ·»åŠ æœ€åä¸€ä¸ªç¼ºå£
        gaps.append((gap_start, gap_end + timedelta(minutes=interval_min)))

        return gaps

    def backfill_candles(
        self, symbol: str, interval: str, start_time: datetime, end_time: datetime
    ) -> Tuple[int, Optional[str]]:
        """
        åˆ†æ‰¹å›å¡«Kçº¿æ•°æ® - æ”¯æŒå¤§é‡å†å²æ•°æ®ä¸‹è½½

        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            interval: æ—¶é—´é—´éš”
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´

        Returns:
            (æ·»åŠ çš„è®°å½•æ•°, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # è½¬æ¢ä¸ºBinanceç¬¦å·æ ¼å¼
            binance_symbol = self.convert_symbol_for_binance(symbol)
            binance_interval = self.convert_interval_for_binance(interval)

            total_days = (end_time - start_time).days
            logger.info(f"ğŸ”„ å¼€å§‹å›å¡« {symbol} {interval}ï¼Œæ—¶é—´èŒƒå›´: {total_days} å¤©")
            logger.info(f"   ä» {start_time.strftime('%Y-%m-%d %H:%M')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M')}")

            # åˆ†æ‰¹ç­–ç•¥ï¼šæ ¹æ®æ—¶é—´é—´éš”å†³å®šæ¯æ‰¹çš„å¤©æ•°
            batch_days = {
                "1d": 365,  # æ—¥çº¿ä¸€æ¬¡è·å–1å¹´
                "4h": 180,  # 4å°æ—¶çº¿ä¸€æ¬¡è·å–åŠå¹´
                "30m": 30,  # 30åˆ†é’Ÿçº¿ä¸€æ¬¡è·å–1ä¸ªæœˆ
                "5m": 7,  # 5åˆ†é’Ÿçº¿ä¸€æ¬¡è·å–1å‘¨
                "1m": 3,  # 1åˆ†é’Ÿçº¿ä¸€æ¬¡è·å–3å¤©
            }.get(interval, 30)

            total_records = 0
            current_start = start_time
            ch_client = clickhouse_connect.get_client(**self.ch_config)

            try:
                while current_start < end_time:
                    # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ç»“æŸæ—¶é—´
                    current_end = min(current_start + timedelta(days=batch_days), end_time)

                    logger.info(f"ğŸ“¦ æ‰¹æ¬¡: {current_start.strftime('%Y-%m-%d')} åˆ° {current_end.strftime('%Y-%m-%d')}")

                    # è°ƒç”¨Binance API
                    start_ms = int(current_start.timestamp() * 1000)
                    end_ms = int(current_end.timestamp() * 1000)

                    url = "https://fapi.binance.com/fapi/v1/klines"
                    params = {
                        "symbol": binance_symbol,
                        "interval": binance_interval,
                        "startTime": start_ms,
                        "endTime": end_ms,
                        "limit": 1500,
                    }

                    response = requests.get(url, params=params, timeout=30)

                    if response.status_code != 200:
                        error_msg = f"Binance API é”™è¯¯: {response.status_code} - {response.text}"
                        logger.error(error_msg)
                        return total_records, error_msg

                    klines_data = response.json()
                    logger.info(f"  ğŸ“Š API è¿”å› {len(klines_data)} æ¡æ•°æ®")

                    if klines_data:
                        # å‡†å¤‡æ’å…¥æ•°æ®
                        insert_data = []
                        for kline in klines_data:
                            open_time = datetime.fromtimestamp(kline[0] / 1000)

                            # æ„é€ æ ‡å‡†åŒ–æ•°æ®
                            raw_data = {
                                "timestamp": open_time,
                                "exchange": "binance",  # REST APIåŸå§‹åç§°
                                "symbol": symbol,
                                "interval": interval,
                                "open": float(kline[1]),
                                "high": float(kline[2]),
                                "low": float(kline[3]),
                                "close": float(kline[4]),
                                "volume": float(kline[5]),
                                "trades": int(kline[8]) if len(kline) > 8 else 0,  # äº¤æ˜“æ¬¡æ•°ï¼Œé»˜è®¤0
                            }

                            # æ•°æ®æ ‡å‡†åŒ–å¤„ç†
                            normalized_data = normalize_data(raw_data, "candle")

                            insert_data.append(
                                [
                                    normalized_data["timestamp"],  # timestamp
                                    normalized_data["exchange"],  # exchange (æ ‡å‡†åŒ–åçš„BINANCE_FUTURES)
                                    normalized_data["symbol"],  # symbol
                                    normalized_data["interval"],  # interval
                                    normalized_data["open"],  # open
                                    normalized_data["high"],  # high
                                    normalized_data["low"],  # low
                                    normalized_data["close"],  # close
                                    normalized_data["volume"],  # volume
                                    normalized_data.get("trades", 0),  # trades äº¤æ˜“æ¬¡æ•°ï¼Œé»˜è®¤0
                                ]
                            )

                        # æ‰¹é‡æ’å…¥åˆ°ClickHouse - æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                        if insert_data:
                            # è°ƒè¯•ï¼šæ£€æŸ¥ç¬¬ä¸€è¡Œæ•°æ®çš„ç»“æ„
                            first_row = insert_data[0]
                            logger.debug(f"ğŸ” ç¬¬ä¸€è¡Œæ•°æ®é•¿åº¦: {len(first_row)}")
                            logger.debug(f"ğŸ” ç¬¬ä¸€è¡Œæ•°æ®ç±»å‹: {[type(x).__name__ for x in first_row]}")

                            # æ£€æŸ¥æ˜¯å¦æœ‰Noneå€¼
                            for i, value in enumerate(first_row):
                                if value is None:
                                    logger.error(f"ğŸš¨ å‘ç°Noneå€¼åœ¨ç¬¬ {i} åˆ—")

                            # ä½¿ç”¨INSERT OR REPLACEé¿å…é‡å¤æ•°æ®
                            try:
                                # ä½¿ç”¨ClickHouseçš„ReplacingMergeTreeç‰¹æ€§ï¼Œé¿å…é‡å¤æ’å…¥
                                # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ•°æ®
                                first_time = insert_data[0][0]  # timestamp
                                last_time = insert_data[-1][0]  # timestamp

                                check_query = f"""
                                SELECT COUNT(*) FROM candles
                                WHERE symbol = '{symbol}' AND interval = '{interval}'
                                AND timestamp >= '{first_time}' AND timestamp <= '{last_time}'
                                """
                                result = ch_client.query(check_query)
                                existing_count = result.result_rows[0][0] if result.result_rows else 0

                                if existing_count > 0:
                                    logger.warning(f"âš ï¸ å‘ç° {existing_count} æ¡é‡å¤æ•°æ®ï¼Œè·³è¿‡æ’å…¥ {symbol} {interval}")
                                    skipped_records = len(insert_data)
                                else:
                                    # ç›´æ¥æ’å…¥9ä¸ªå­—æ®µï¼Œä¸è¡¨ç»“æ„å®Œå…¨åŒ¹é…
                                    ch_client.insert("candles", insert_data)
                                    logger.info(f"  âœ… æ’å…¥ {len(insert_data)} æ¡æ•°æ®")
                                    skipped_records = 0

                            except Exception as e:
                                logger.error(f"ğŸš¨ ClickHouseæ’å…¥å¤±è´¥: {e}")
                                logger.error(f"ğŸ” æ’å…¥æ•°æ®æ ·æœ¬ (å‰3è¡Œ):")
                                for i, row in enumerate(insert_data[:3]):
                                    logger.error(f"  è¡Œ{i}: é•¿åº¦={len(row)}, æ•°æ®={row}")
                                raise e
                        else:
                            logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥")
                        total_records += len(insert_data)

                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
                    current_start = current_end

                    # APIé™åˆ¶ï¼šå»¶è¿Ÿé¿å…è§¦å‘é™åˆ¶
                    time.sleep(0.2)

                logger.info(f"ğŸ‰ å›å¡«å®Œæˆï¼{symbol} {interval} æ€»è®¡æ’å…¥ {total_records} æ¡æ•°æ®")
                return total_records, None

            finally:
                ch_client.close()

        except Exception as e:
            error_msg = f"å›å¡«å¤±è´¥ {symbol} {interval}: {str(e)}"
            logger.error(error_msg)
            return 0, error_msg

    def convert_symbol_for_binance(self, symbol: str) -> str:
        """è½¬æ¢cryptofeedç¬¦å·æ ¼å¼ä¸ºBinanceæ ¼å¼"""
        if symbol.endswith("-PERP"):
            return symbol.replace("-USDT-PERP", "USDT").replace("-", "")
        return symbol.replace("-", "")

    def convert_interval_for_binance(self, interval: str) -> str:
        """è½¬æ¢æ—¶é—´é—´éš”æ ¼å¼"""
        mapping = {"1m": "1m", "5m": "5m", "30m": "30m", "4h": "4h", "1d": "1d"}
        return mapping.get(interval, interval)

    def run_backfill_tasks(self, symbols: List[str], lookback_days: int = None) -> Dict[str, any]:
        """
        è¿è¡Œå†å²æ•°æ®å›å¡«ä»»åŠ¡

        Args:
            symbols: è¦å¤„ç†çš„äº¤æ˜“å¯¹åˆ—è¡¨
            lookback_days: æ£€æŸ¥æœ€è¿‘Nå¤©çš„æ•°æ®

        Returns:
            å›å¡«ç»“æœç»Ÿè®¡
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šlookback_daysï¼Œä»é…ç½®ä¸­è¯»å–
        if lookback_days is None:
            lookback_days = config.get("data_backfill.default_lookback_days", 7)

        logger.info(f"Starting backfill for {len(symbols)} symbols, lookback {lookback_days} days")

        # æ£€æµ‹æ•°æ®ç¼ºå£
        tasks = self.detect_data_gaps(symbols, lookback_days)

        if not tasks:
            logger.info("No data gaps detected")
            return {"total_tasks": 0, "successful": 0, "failed": 0, "records_added": 0}

        # æ‰§è¡Œå›å¡«ä»»åŠ¡
        successful_tasks = 0
        failed_tasks = 0
        total_records = 0
        failed_symbols = []

        for task in tasks:
            try:
                logger.info(f"Processing backfill task: {task.symbol} {task.interval}")

                records_added, error_msg = self.backfill_candles(
                    task.symbol, task.interval, task.start_time, task.end_time
                )

                if error_msg:
                    logger.error(f"Backfill failed for {task.symbol} {task.interval}: {error_msg}")
                    failed_tasks += 1
                    failed_symbols.append(f"{task.symbol} {task.interval}")
                else:
                    successful_tasks += 1
                    total_records += records_added
                    logger.info(f"Backfill completed for {task.symbol} {task.interval}: {records_added} records")

                # é¿å…APIé™åˆ¶
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"Unexpected error during backfill {task.symbol} {task.interval}: {e}")
                failed_tasks += 1
                failed_symbols.append(f"{task.symbol} {task.interval}")

        result = {
            "total_tasks": len(tasks),
            "successful": successful_tasks,
            "failed": failed_tasks,
            "records_added": total_records,
            "failed_symbols": failed_symbols,
        }

        logger.info(f"Backfill completed: {successful_tasks}/{len(tasks)} successful, {total_records} records added")
        if failed_symbols:
            logger.warning(f"Failed symbols: {', '.join(failed_symbols)}")

        return result

    def run_continuous_backfill(self, symbols: List[str], check_interval_hours: int = 6):
        """
        æŒç»­è¿è¡Œæ•°æ®è¡¥å……æœåŠ¡

        Args:
            symbols: è¦ç›‘æ§çš„äº¤æ˜“å¯¹åˆ—è¡¨
            check_interval_hours: æ£€æŸ¥é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        logger.info(
            f"Starting continuous backfill service for {len(symbols)} symbols (check every {check_interval_hours} hours)"
        )

        while True:
            try:
                # è¿è¡Œä¸€è½®å›å¡«ä»»åŠ¡
                default_lookback = config.get("data_backfill.default_lookback_days", 7)
                result = self.run_backfill_tasks(symbols, lookback_days=default_lookback)

                if result["total_tasks"] > 0:
                    logger.info(
                        f"Backfill cycle: {result['successful']}/{result['total_tasks']} tasks successful, {result['records_added']} records added"
                    )

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                time.sleep(check_interval_hours * 3600)

            except Exception as e:
                logger.error(f"Error in continuous backfill: {e}")
                time.sleep(300)  # é”™è¯¯æ—¶ç­‰å¾…5åˆ†é’Ÿå†é‡è¯•

    def _get_interval_minutes(self, interval: str) -> int:
        """è·å–æ—¶é—´é—´éš”çš„åˆ†é’Ÿæ•°"""
        interval_mapping = {"1m": 1, "5m": 5, "15m": 15, "30m": 30, "1h": 60, "4h": 240, "1d": 1440}
        return interval_mapping.get(interval, 60)
